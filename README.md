# Project Ignite 2018 Video Game AI Group

This was a project done by Otis Smith, Jack Cameron, Will Aracri, Kyle Carskadden, Trey Natili, and Thomas Chang over a ten week period through Carnegie Mellon University's Project Ignite, with the assistance of Brian Scheuermann, Tiffany Ma, and Tyler Sikov. The project was presented at a showcase event at CMU in April, 2018. 

The project was split into two main portions, programming an alpha-beta algorithm to play Connect 4, and using reinforcement learning to play simple simulations (CartPole and Pendulum) from OpenAI's Gym. Although both portions are present in this repository, only the neural network section will be discussed in the readme. 

## Prerequisites

Before trying to run any of the files, make sure that Python 3, TensorFlow, and OpenAI Gym are all installed properly. See the individual product websites for each for installation instructions. 

## Background on OpenAI Gym Simulations

Although much of this information can be found on [OpenAI Gym's website](https://gym.openai.com/docs/), it is worth explaining exactly what is going on. On the Gym website, there are many simple simulations, each of which are able to be run quickly and make collecting data for use with a neural network quite easy. Looking into the documentation reveals more information about how to access this information, which was implemented in the code. For this project, two simulations were used: CartPole and Pendulum. Each of these consists of a simple left/right input to the simulation, where the goal is to either balance a pole on a cart or to keep a spinning object upright for as much time as possible, respectively. The neural network acts in this scenario to maximize reward and minimize loss for the overall simulation. 

## Using These Files

Given that much of the work relies solely on Python, TensorFlow, and Gym, little else is needed to run the files if these three are properly installed. Simply download the repository and run the .py files in the folder titled OpenAI_Gym_TensorFlow. Running these files anywhere will work, but running them in the command line shell outputs the data for epoch and loss best when the neural network is training.  

The file named "CartPole.py" runs a few random input tests in a window, collects data, and trains a neural network based on those trials which scored well (according to the built in reward system in Gym). The second set of trials is the outputs generated by the neural network after it has been trained. Because the pole balances much better, the neural network was deemed successful in optimizing this situation.

The file named "pendulum.py" runs similarly, with the only difference being that the simulation is now the Pendulum simulation. The result after training is only marginally better, unfortunately, but does seem to have some improvement over random input. With more training time and adjustment of the neural network, results could likely be improved. 


## Built With

* [TensorFlow](https://www.tensorflow.org/) - The neural network Python library used
* [OpenAI Gym](https://gym.openai.com/docs/) - Simulations used

## Authors

* **Otis Smith** - *Working on TensorFlow portion and writing README* - [themroats](https://github.com/themroats)

See also the list of [contributors](https://github.com/brians1123/Ignite_2018) who participated in this project, for whom I do not have updated contact info.


